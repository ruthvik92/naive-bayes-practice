{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd97089-cef8-4c26-91ef-b1e99186060b",
   "metadata": {},
   "source": [
    "* **RAW MLE**\n",
    "* Simply ratio between number of times the $word_{i}$ occurred in a class and total number of words in the class. \n",
    " \n",
    " $P(\\text{word}_i \\mid c) \\;=\\; \\frac{\\text{count}(\\text{word}_i, c)}{\\sum_{w} \\text{count}(w, c)}$\n",
    "\n",
    "* **SMOOTHED MLE**\n",
    "* However, we add a single count for every word's count so that we do not get any zero occurrences between the classes. This is to ensure that all the words occurred at least once in all the classes. Because we have added 1 to the numerator (every single word's count is increased by 1). That is equal to vacb_size so, we must add that to the numerator (AKA Laplace Smoothing).\n",
    "  \n",
    "$P(\\text{word}_i \\mid c) \\;=\\; \n",
    "\\frac{\\text{count}(\\text{word}_i, c) + 1}{\n",
    "   \\sum_{w} \\text{count}(w, c) \\;+\\; \\lvert V \\rvert}$\n",
    "\n",
    "* We know that $Posterior \\propto Prior \\times Likelihood$\n",
    "* Prior is simply the ratio of number of samples in a class and total number of samples (typically training set)\n",
    "\n",
    " $P(c) \\;=\\; \\frac{\\text{count}(c)}{\\sum_{c'} \\text{count}(c')}$\n",
    " \n",
    " * Probability of a sentence belonging to a class (sentence is $w_1, w_2, \\ldots, w_n$). The whole point of Naive bayes is that we simply take the products of these likelihoods as if they're independent (many times they're not!!). See the notebook at `naive-bayes-practice/naive-bayes-gaussian/naive_bayes_scratch.ipynb` for more on Naive Bayes.\n",
    "   \n",
    " $P(c \\mid w_1, w_2, \\ldots, w_n)\n",
    "\\;\\propto\\;\n",
    "P(c)\n",
    "\\;\\prod_{i=1}^{n}\n",
    "P\\bigl(w_i \\mid c\\bigr)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98f0b25-95c4-4d5c-b277-2ae3d97c0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n",
      "Test samples: 2\n",
      "Vocabulary size: 34\n",
      "Accuracy on test set = 50.00%\n",
      "\n",
      "Message: 'Hurry! Free cash prize if you call now' -> Predicted Label: spam\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------------------------\n",
    "# 1. A small SMS dataset\n",
    "# ---------------------------\n",
    "sample_sms_data = [\n",
    "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts!!\", \"spam\"),\n",
    "    (\"U dun say so early hor... U c already then say...\", \"ham\"),\n",
    "    (\"Nah I don't think he goes to usf, he lives around here though\", \"ham\"),\n",
    "    (\"Free tickets available, exclusive offer just for you!\", \"spam\"),\n",
    "    (\"Win cash now!!! Just send WIN to 12345\", \"spam\"),\n",
    "    (\"Hello, hope you are doing well my friend.\", \"ham\")\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Train/Test Split\n",
    "# ---------------------------\n",
    "def train_test_split(data, test_size=0.33, seed=42):\n",
    "    \"\"\" Shuffle and split into train and test sets. \"\"\"\n",
    "    random.seed(seed)\n",
    "    shuffled = data[:]\n",
    "    random.shuffle(shuffled)\n",
    "    split_point = int(len(shuffled) * (1 - test_size))\n",
    "    return shuffled[:split_point], shuffled[split_point:]\n",
    "\n",
    "train_data, test_data = train_test_split(sample_sms_data, test_size=0.33)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Preprocessing Functions\n",
    "# ---------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - Lowercase\n",
    "    - Remove non-alphanumeric (punctuation)\n",
    "    - Tokenize (split on whitespace)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # remove punctuation using regex\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(dataset):\n",
    "    \"\"\"\n",
    "    Build a vocabulary set of all unique words from the training data.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sms, label in dataset:\n",
    "        tokens = preprocess_text(sms)\n",
    "        for t in tokens:\n",
    "            vocab.add(t)\n",
    "    return sorted(list(vocab))  # sorted list (optional)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train a Multinomial NB Model\n",
    "# ---------------------------\n",
    "def train_multinomial_nb(train_data):\n",
    "    \"\"\"\n",
    "    Train a Multinomial Naive Bayes:\n",
    "      - Compute P(class) (priors)\n",
    "      - Compute P(word | class) for each word\n",
    "    Returns model parameters:\n",
    "      - class_priors: dict -> {class_label: prior_probability}\n",
    "      - word_counts: dict -> {class_label: {word: count}}\n",
    "      - total_words_in_class: dict -> {class_label: total_count_of_words}\n",
    "      - vocabulary\n",
    "    \"\"\"\n",
    "    # Count how many docs per class\n",
    "    class_counts = defaultdict(int)  # {spam: X, ham: Y}\n",
    "    \n",
    "    # Word counts per class: {spam: {word: count}, ham: {word: count}}\n",
    "    word_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sms, label in train_data:\n",
    "        class_counts[label] += 1\n",
    "        tokens = preprocess_text(sms)\n",
    "        for token in tokens:\n",
    "            word_counts[label][token] += 1\n",
    "\n",
    "    # Compute total docs\n",
    "    total_docs = sum(class_counts.values())\n",
    "    \n",
    "    # Compute priors: P(class)\n",
    "    class_priors = {}\n",
    "    for c in class_counts:\n",
    "        class_priors[c] = class_counts[c] / total_docs\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    vocabulary = build_vocabulary(train_data)\n",
    "\n",
    "    # Compute total words in each class\n",
    "    total_words_in_class = {}\n",
    "    for c in class_counts:\n",
    "        total_words_in_class[c] = sum(word_counts[c].values())\n",
    "\n",
    "    # Return a dictionary holding all necessary info\n",
    "    model = {\n",
    "        'class_priors': class_priors,\n",
    "        'word_counts': dict(word_counts),\n",
    "        'total_words_in_class': total_words_in_class,\n",
    "        'vocabulary': vocabulary,\n",
    "        'class_counts': class_counts\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def predict_class(model, text):\n",
    "    \"\"\"\n",
    "    Predict class for a given text using the trained Multinomial NB model.\n",
    "    We do:\n",
    "       P(c | doc) ~ P(c) * Î  [ P(word_i | c) ]\n",
    "    Where P(word_i | c) = (count(word_i, c) + 1) / (total_words_in_class[c] + vocab_size)\n",
    "    (We apply Laplace smoothing by adding 1).\n",
    "   log(P(c | doc)) =  log(P(c)) + Sum[ P(word_i | c) ]\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    class_priors = model['class_priors']\n",
    "    word_counts = model['word_counts']\n",
    "    total_words_in_class = model['total_words_in_class']\n",
    "    vocabulary = model['vocabulary']\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    # Calculate posterior for each class\n",
    "    scores = {}\n",
    "    for c in class_priors:\n",
    "        # Start with log(P(c))\n",
    "        log_prob = math.log(class_priors[c])\n",
    "        \n",
    "        # For each word in the SMS\n",
    "        for w in tokens:\n",
    "            # Laplace smoothing\n",
    "            count_wc = word_counts[c].get(w, 0)\n",
    "            p_w_given_c = (count_wc + 1) / (total_words_in_class[c] + vocab_size)\n",
    "            # use log probabilities to avoid underflow\n",
    "            log_prob += math.log(p_w_given_c)\n",
    "        \n",
    "        scores[c] = log_prob\n",
    "    \n",
    "    # pick the class with highest log-prob\n",
    "    predicted_class = max(scores, key=scores.get)\n",
    "    return predicted_class\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy on test set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for sms, label in test_data:\n",
    "        pred = predict_class(model, sms)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct / len(test_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Putting it all together\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Train\n",
    "    model = train_multinomial_nb(train_data)\n",
    "    \n",
    "    # Test\n",
    "    accuracy = evaluate(model, test_data)\n",
    "    \n",
    "    # Print some results\n",
    "    print(\"Training samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Vocabulary size:\", len(model['vocabulary']))\n",
    "    print(f\"Accuracy on test set = {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Demo prediction\n",
    "    test_sms = \"Hurry! Free cash prize if you call now\"\n",
    "    predicted_label = predict_class(model, test_sms)\n",
    "    print(f\"\\nMessage: '{test_sms}' -> Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3326d0-cfee-4d34-91e1-7cbc27d2a88b",
   "metadata": {},
   "source": [
    "# Using a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44bfe42-b7e0-44d1-8b80-b829ab8cb231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load data\n",
    "train_df = pd.read_csv('sms+spam+collection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead58641-3bd0-468b-92a5-042ea3eeb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_df.values.tolist()\n",
    "all_data = [[sample[1], sample[0]] for sample in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b528cbb3-65a5-4630-830c-1adaf9a61749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(all_data, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cb07648-e371-4be9-8aa2-b76b1bd4fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm really not up to it still tonight babe\", 'ham'] ['URGENT! Your Mobile number has been awarded a <UKP>2000 prize GUARANTEED. Call 09061790125 from landline. Claim 3030. Valid 12hrs only 150ppm', 'spam']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0],test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "628aa8da-64b5-4583-aafb-ab2c9b86472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3733\n",
      "Test samples: 1839\n",
      "Vocabulary size: 7594\n",
      "Accuracy on test set = 97.50%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = train_multinomial_nb(train_data)\n",
    "\n",
    "# Test\n",
    "accuracy = evaluate(model, test_data)\n",
    "\n",
    "# Print some results\n",
    "print(\"Training samples:\", len(train_data))\n",
    "print(\"Test samples:\", len(test_data))\n",
    "print(\"Vocabulary size:\", len(model['vocabulary']))\n",
    "print(f\"Accuracy on test set = {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe47cc7-d597-42d2-8bae-53210eb7ebf3",
   "metadata": {},
   "source": [
    "# Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9ed4356-80c8-43e7-9ea8-bfd31062c47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1593\n",
      "        spam       0.95      0.96      0.95       246\n",
      "\n",
      "    accuracy                           0.99      1839\n",
      "   macro avg       0.97      0.97      0.97      1839\n",
      "weighted avg       0.99      0.99      0.99      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv('sms+spam+collection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "\n",
    "# 2. Encode labels (spam=1, ham=0) if needed\n",
    "df['label_encoded'] = df['label'].map({'spam':1, 'ham':0})\n",
    "\n",
    "# 3. Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['message'], \n",
    "    df['label_encoded'], \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Convert text to feature vectors (bag-of-words)\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Train Naive Bayes model\n",
    "model = MultinomialNB(alpha=1)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "\n",
    "# 6. Predict on test set\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "\n",
    "# 7. Evaluate\n",
    "print(classification_report(y_test, y_pred, target_names=['ham','spam']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a4203-11c6-49bd-b367-48e7f2761839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
