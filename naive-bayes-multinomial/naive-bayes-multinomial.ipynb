{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd97089-cef8-4c26-91ef-b1e99186060b",
   "metadata": {},
   "source": [
    "* **RAW MLE**\n",
    "* Simply ratio between number of times the $word_{i}$ occurred in a class and total number of words in the class. \n",
    " \n",
    " $P(\\text{word}_i \\mid c) \\;=\\; \\frac{\\text{count}(\\text{word}_i, c)}{\\sum_{w} \\text{count}(w, c)}$\n",
    "\n",
    "* **SMOOTHED MLE**\n",
    "* However, we add a single count for every word's count so that we do not get any zero occurrences between the classes. This is to ensure that all the words occurred at least once in all the classes. Because we have added 1 to the numerator (every single word's count is increased by 1). That is equal to vacb_size so, we must add that to the numerator (AKA Laplace Smoothing).\n",
    "  \n",
    "$P(\\text{word}_i \\mid c) \\;=\\; \n",
    "\\frac{\\text{count}(\\text{word}_i, c) + 1}{\n",
    "   \\sum_{w} \\text{count}(w, c) \\;+\\; \\lvert V \\rvert}$\n",
    "\n",
    "* We know that $Posterior \\propto Prior \\times Likelihood$\n",
    "* Prior is simply the ratio of number of samples in a class and total number of samples (typically training set)\n",
    "\n",
    " $P(c) \\;=\\; \\frac{\\text{count}(c)}{\\sum_{c'} \\text{count}(c')}$\n",
    " \n",
    " * Probability of a sentence belonging to a class (sentence is $w_1, w_2, \\ldots, w_n$). The whole point of Naive bayes is that we simply take the products of these likelihoods as if they're independent (many times they're not!!). See the notebook at `naive-bayes-practice/naive-bayes-gaussian/naive_bayes_scratch.ipynb` for more on Naive Bayes.\n",
    "   \n",
    " $P(c \\mid w_1, w_2, \\ldots, w_n)\n",
    "\\;\\propto\\;\n",
    "P(c)\n",
    "\\;\\prod_{i=1}^{n}\n",
    "P\\bigl(w_i \\mid c\\bigr)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98f0b25-95c4-4d5c-b277-2ae3d97c0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n",
      "Test samples: 2\n",
      "Vocabulary size: 34\n",
      "Accuracy on test set = 50.00%\n",
      "\n",
      "Message: 'Hurry! Free cash prize if you call now' -> Predicted Label: spam\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------------------------\n",
    "# 1. A small SMS dataset\n",
    "# ---------------------------\n",
    "sample_sms_data = [\n",
    "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts!!\", \"spam\"),\n",
    "    (\"U dun say so early hor... U c already then say...\", \"ham\"),\n",
    "    (\"Nah I don't think he goes to usf, he lives around here though\", \"ham\"),\n",
    "    (\"Free tickets available, exclusive offer just for you!\", \"spam\"),\n",
    "    (\"Win cash now!!! Just send WIN to 12345\", \"spam\"),\n",
    "    (\"Hello, hope you are doing well my friend.\", \"ham\")\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Train/Test Split\n",
    "# ---------------------------\n",
    "def train_test_split(data, test_size=0.33, seed=42):\n",
    "    \"\"\" Shuffle and split into train and test sets. \"\"\"\n",
    "    random.seed(seed)\n",
    "    shuffled = data[:]\n",
    "    random.shuffle(shuffled)\n",
    "    split_point = int(len(shuffled) * (1 - test_size))\n",
    "    return shuffled[:split_point], shuffled[split_point:]\n",
    "\n",
    "train_data, test_data = train_test_split(sample_sms_data, test_size=0.33)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Preprocessing Functions\n",
    "# ---------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - Lowercase\n",
    "    - Remove non-alphanumeric (punctuation)\n",
    "    - Tokenize (split on whitespace)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # remove punctuation using regex\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(dataset):\n",
    "    \"\"\"\n",
    "    Build a vocabulary set of all unique words from the training data.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sms, label in dataset:\n",
    "        tokens = preprocess_text(sms)\n",
    "        for t in tokens:\n",
    "            vocab.add(t)\n",
    "    return sorted(list(vocab))  # sorted list (optional)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train a Multinomial NB Model\n",
    "# ---------------------------\n",
    "def train_multinomial_nb(train_data):\n",
    "    \"\"\"\n",
    "    Train a Multinomial Naive Bayes:\n",
    "      - Compute P(class) (priors)\n",
    "      - Compute P(word | class) for each word\n",
    "    Returns model parameters:\n",
    "      - class_priors: dict -> {class_label: prior_probability}\n",
    "      - word_counts: dict -> {class_label: {word: count}}\n",
    "      - total_words_in_class: dict -> {class_label: total_count_of_words}\n",
    "      - vocabulary\n",
    "    \"\"\"\n",
    "    # Count how many docs per class\n",
    "    class_counts = defaultdict(int)  # {spam: X, ham: Y}\n",
    "    \n",
    "    # Word counts per class: {spam: {word: count}, ham: {word: count}}\n",
    "    word_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sms, label in train_data:\n",
    "        class_counts[label] += 1\n",
    "        tokens = preprocess_text(sms)\n",
    "        for token in tokens:\n",
    "            word_counts[label][token] += 1\n",
    "\n",
    "    # Compute total docs\n",
    "    total_docs = sum(class_counts.values())\n",
    "    \n",
    "    # Compute priors: P(class)\n",
    "    class_priors = {}\n",
    "    for c in class_counts:\n",
    "        class_priors[c] = class_counts[c] / total_docs\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    vocabulary = build_vocabulary(train_data)\n",
    "\n",
    "    # Compute total words in each class\n",
    "    total_words_in_class = {}\n",
    "    for c in class_counts:\n",
    "        total_words_in_class[c] = sum(word_counts[c].values())\n",
    "\n",
    "    # Return a dictionary holding all necessary info\n",
    "    model = {\n",
    "        'class_priors': class_priors,\n",
    "        'word_counts': dict(word_counts),\n",
    "        'total_words_in_class': total_words_in_class,\n",
    "        'vocabulary': vocabulary,\n",
    "        'class_counts': class_counts\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def predict_class(model, text):\n",
    "    \"\"\"\n",
    "    Predict class for a given text using the trained Multinomial NB model.\n",
    "    We do:\n",
    "       P(c | doc) ~ P(c) * Î  [ P(word_i | c) ]\n",
    "    Where P(word_i | c) = (count(word_i, c) + 1) / (total_words_in_class[c] + vocab_size)\n",
    "    (We apply Laplace smoothing by adding 1).\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    class_priors = model['class_priors']\n",
    "    word_counts = model['word_counts']\n",
    "    total_words_in_class = model['total_words_in_class']\n",
    "    vocabulary = model['vocabulary']\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    # Calculate posterior for each class\n",
    "    scores = {}\n",
    "    for c in class_priors:\n",
    "        # Start with log(P(c))\n",
    "        log_prob = math.log(class_priors[c])\n",
    "        \n",
    "        # For each word in the SMS\n",
    "        for w in tokens:\n",
    "            # Laplace smoothing\n",
    "            count_wc = word_counts[c].get(w, 0)\n",
    "            p_w_given_c = (count_wc + 1) / (total_words_in_class[c] + vocab_size)\n",
    "            # use log probabilities to avoid underflow\n",
    "            log_prob += math.log(p_w_given_c)\n",
    "        \n",
    "        scores[c] = log_prob\n",
    "    \n",
    "    # pick the class with highest log-prob\n",
    "    predicted_class = max(scores, key=scores.get)\n",
    "    return predicted_class\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy on test set.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for sms, label in test_data:\n",
    "        pred = predict_class(model, sms)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct / len(test_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Putting it all together\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Train\n",
    "    model = train_multinomial_nb(train_data)\n",
    "    \n",
    "    # Test\n",
    "    accuracy = evaluate(model, test_data)\n",
    "    \n",
    "    # Print some results\n",
    "    print(\"Training samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Vocabulary size:\", len(model['vocabulary']))\n",
    "    print(f\"Accuracy on test set = {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Demo prediction\n",
    "    test_sms = \"Hurry! Free cash prize if you call now\"\n",
    "    predicted_label = predict_class(model, test_sms)\n",
    "    print(f\"\\nMessage: '{test_sms}' -> Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459012b9-49e9-4d31-8d4a-f40eaaef92b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
